# backend/direct_upload.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Direct SQLite ‚Üí Qdrant Cloud uploader (streaming, resilient)

Why this version?
- Streams rows out of SQLite with fetchmany() (no giant .fetchall())
- Converts pickled Qdrant records into dense vectors + payload
- Uploads in batches with retries/backoff
- Auto-creates/optionally-replaces the cloud collection
- Detects vector dimension from the first valid row (or accept --dim)
- Verifies upload without forcing heavyweight model downloads
- Keeps backward-compat helpers: extract_points_from_sqlite(), upload_points_to_cloud()

Typical usage:
  python -m backend.direct_upload --replace
  python -m backend.direct_upload --db ed_index_full/storage.sqlite --batch 500
  python -m backend.direct_upload --collection ed_chunks_v2 --dim 384

Environment:
  QDRANT_URL
  QDRANT_API_KEY
"""

from __future__ import annotations

import os
import sys
import argparse
import sqlite3
import pickle
import uuid
import time
from typing import Generator, Iterable, List, Optional, Tuple, Dict, Any

from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct

# Optional: light verify search if embeddings are available (not required)
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings  # type: ignore
except Exception:  # pragma: no cover
    HuggingFaceEmbeddings = None  # type: ignore

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL") or ""
QDRANT_API = os.getenv("QDRANT_API_KEY") or ""
DEFAULT_COLLECTION = "ed_chunks"
DEFAULT_SQLITE = "ed_index_full/storage.sqlite"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Utilities
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _eprint(*a, **k):
    print(*a, file=sys.stderr, **k)


def _vector_from_any(vec: Any) -> Optional[List[float]]:
    """
    Convert a variety of vector shapes to a plain list[float].
    Supports:
      - list[float]
      - dict with a first named dense vector
      - objects carrying `.values` and `.indices` (sparse) ‚Üí skipped (no safe dim)
    """
    if vec is None:
        return None
    if isinstance(vec, list):
        # already dense
        return [float(x) for x in vec]
    if isinstance(vec, dict) and vec:
        # take first named vector if it's a list
        first_key = next(iter(vec))
        v = vec[first_key]
        if isinstance(v, list):
            return [float(x) for x in v]
        # sparse types (values/indices) are ambiguous without a declared dimension;
        # return None so caller can skip or handle separately.
        if hasattr(v, "values") and hasattr(v, "indices"):
            return None
    # objects with .vector
    if hasattr(vec, "__iter__"):
        try:
            return [float(x) for x in list(vec)]  # best-effort
        except Exception:
            return None
    return None


def _decode_pickled_point(raw: bytes) -> Tuple[Optional[str], Optional[List[float]], Dict[str, Any]]:
    """
    Decode a pickled Qdrant local record into (original_id, vector, payload).
    If vector cannot be resolved, returns (id, None, payload).
    NOTE: This trusts local SQLite generated by your own pipeline.
    """
    obj = pickle.loads(raw)  # noqa: S301 (trusted local artifact)
    original_id = None
    payload: Dict[str, Any] = {}

    # id
    if hasattr(obj, "id"):
        original_id = obj.id
    elif isinstance(obj, dict) and "id" in obj:
        original_id = obj["id"]
    elif hasattr(obj, "__dict__") and "id" in obj.__dict__:
        original_id = obj.__dict__["id"]

    # payload
    if isinstance(obj, dict) and "payload" in obj:
        payload = obj.get("payload") or {}
    elif hasattr(obj, "payload"):
        payload = getattr(obj, "payload", {}) or {}
    elif hasattr(obj, "__dict__") and "payload" in obj.__dict__:
        payload = obj.__dict__["payload"] or {}

    # vector
    cand = None
    if isinstance(obj, dict) and "vector" in obj:
        cand = obj["vector"]
    elif hasattr(obj, "vector") and not isinstance(obj, dict):
        cand = obj.vector
    elif hasattr(obj, "__dict__") and "vector" in obj.__dict__:
        cand = obj.__dict__["vector"]

    vec = _vector_from_any(cand)
    if original_id is not None:
        payload.setdefault("original_id", original_id)
    return str(original_id) if original_id is not None else None, vec, payload


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SQLite streaming extractor
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def iter_points_from_sqlite(
    db_path: str = DEFAULT_SQLITE,
    fetch_size: int = 2000,
    max_rows: Optional[int] = None,
) -> Generator[PointStruct, None, int]:
    """
    Stream rows from the local SQLite 'points' table and yield PointStructs.
    Returns the total attempted rows as the generator's return value.
    """
    if not os.path.exists(db_path):
        _eprint(f"‚ùå SQLite file not found: {db_path}")
        return 0  # type: ignore [return-value]

    con = sqlite3.connect(db_path)
    try:
        cur = con.cursor()
        # Count total (for progress)
        try:
            cur.execute("SELECT COUNT(*) FROM points")
            total = int(cur.fetchone()[0] or 0)
        except Exception:
            total = 0

        print(f"üìä Reading from: {db_path}")
        print(f"üìà Points in table: {total or 'unknown'}")
        cur.execute("SELECT id, point FROM points")

        processed = 0
        yielded = 0
        failures = 0

        while True:
            rows = cur.fetchmany(fetch_size)
            if not rows:
                break

            for _row_id, raw in rows:
                processed += 1
                if max_rows and processed > max_rows:
                    break
                try:
                    _orig, vec, payload = _decode_pickled_point(raw)
                    if not isinstance(vec, list) or not vec:
                        failures += 1
                        continue
                    pt = PointStruct(
                        id=str(uuid.uuid4()),     # decouple from local ids
                        vector=vec,
                        payload=payload or {},
                    )
                    yielded += 1
                    if yielded % 1000 == 0:
                        print(f"   ‚Ä¢ yielded {yielded} points‚Ä¶")
                    yield pt
                except Exception as e:
                    failures += 1
                    if failures <= 5:
                        _eprint(f"   ‚ö†Ô∏è decode failure (#{failures}): {e}")
            if max_rows and processed > max_rows:
                break

        print(f"‚úÖ Extraction done: yielded={yielded}, failed={failures}, processed={processed}")
        return processed  # type: ignore [return-value]
    finally:
        con.close()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cloud helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _ensure_collection(
    cli: QdrantClient,
    name: str,
    vector_dim: int,
    replace_existing: bool = False,
) -> None:
    collections = cli.get_collections().collections
    names = [c.name for c in collections]
    if name in names and replace_existing:
        print(f"üóëÔ∏è Deleting existing collection: {name}")
        cli.delete_collection(name)

    if name not in names or replace_existing:
        print(f"üì¶ Creating collection: {name} (dim={vector_dim})")
        cli.create_collection(
            collection_name=name,
            vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),
        )
    else:
        info = cli.get_collection(name)
        current = None
        if info.config and info.config.params and info.config.params.vectors:
            vectors_config = info.config.params.vectors
            if isinstance(vectors_config, dict):
                # Get the first vector config if it's a dictionary
                first_vector = next(iter(vectors_config.values()), None)
                if first_vector and hasattr(first_vector, 'size'):
                    current = int(first_vector.size)
            elif hasattr(vectors_config, 'size'):
                current = int(vectors_config.size)
        
        if current and current != vector_dim:
            raise RuntimeError(
                f"Vector dim mismatch: cloud={current} vs local={vector_dim}. "
                "Re-run with --replace or --dim to match."
            )
        print(f"‚úÖ Collection exists: {name} (dim={current or vector_dim})")


def _batched(iterable: Iterable[PointStruct], batch_size: int) -> Generator[List[PointStruct], None, None]:
    batch: List[PointStruct] = []
    for item in iterable:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch


def _retry_upsert(
    cli: QdrantClient,
    collection: str,
    points: List[PointStruct],
    max_retries: int = 4,
) -> None:
    delay = 0.5
    for attempt in range(1, max_retries + 1):
        try:
            cli.upsert(collection_name=collection, points=points)
            return
        except Exception as e:
            if attempt == max_retries:
                raise
            _eprint(f"   ‚ö†Ô∏è upsert attempt {attempt} failed: {e} ‚Äî retrying in {delay:.1f}s")
            time.sleep(delay)
            delay *= 2


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Legacy-compatible helpers (kept for callers who imported old API)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_points_from_sqlite(db_path: str = DEFAULT_SQLITE, limit: Optional[int] = None) -> List[PointStruct]:
    """
    Back-compat: return a list (not recommended for large datasets).
    Prefer streaming via iter_points_from_sqlite().
    """
    out: List[PointStruct] = []
    for i, pt in enumerate(iter_points_from_sqlite(db_path=db_path, fetch_size=2000, max_rows=limit or None), start=1):
        out.append(pt)
        if limit and i >= limit:
            break
    return out


def upload_points_to_cloud(points: List[PointStruct], collection: str = DEFAULT_COLLECTION, replace_existing: bool = False) -> bool:
    """
    Back-compat: upload a list of points. Prefer streaming in main().
    """
    if not points:
        _eprint("‚ùå No points to upload")
        return False

    if not QDRANT_URL or not QDRANT_API:
        _eprint("‚ùå QDRANT_URL/QDRANT_API_KEY not set")
        return False

    print("üîó Connecting to Qdrant Cloud‚Ä¶")
    cli = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API)
    dim = len(points[0].vector) if isinstance(points[0].vector, list) else 384
    _ensure_collection(cli, collection, dim, replace_existing=replace_existing)

    total = len(points)
    print(f"‚¨ÜÔ∏è Uploading {total} points in batches of 100‚Ä¶")
    uploaded = 0
    for batch in _batched(points, 100):
        _retry_upsert(cli, collection, batch)
        uploaded += len(batch)
        print(f"   ‚Ä¢ {uploaded}/{total}")

    print("‚úÖ Upload complete.")
    return True


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CLI
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main() -> None:
    p = argparse.ArgumentParser(description="Direct SQLite ‚Üí Qdrant Cloud (streaming)")
    p.add_argument("--db", default=DEFAULT_SQLITE, help=f"Path to SQLite (default: {DEFAULT_SQLITE})")
    p.add_argument("--collection", default=DEFAULT_COLLECTION, help=f"Cloud collection (default: {DEFAULT_COLLECTION})")
    p.add_argument("--batch", type=int, default=200, help="Upload batch size (default: 200)")
    p.add_argument("--dim", type=int, default=0, help="Force vector dimension (skip auto-detect)")
    p.add_argument("--replace", action="store_true", help="Replace existing cloud collection")
    p.add_argument("--limit", type=int, default=0, help="Process at most N rows (debug)")
    p.add_argument("--skip-verify", action="store_true", help="Skip post-upload verification")
    args = p.parse_args()

    print("üöÄ DIRECT SQLITE ‚Üí QDRANT CLOUD")
    print("=" * 42)

    if not QDRANT_URL or not QDRANT_API:
        _eprint("‚ùå Missing QDRANT_URL or QDRANT_API_KEY in environment")
        sys.exit(1)

    # Connect to cloud early
    print("üîó Connecting to Qdrant Cloud‚Ä¶")
    cli = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API)
    print("‚úÖ Connected")

    # Stream first N points until we can detect dimension
    dim = int(args.dim) if args.dim and args.dim > 0 else 0
    buffer: List[PointStruct] = []
    yielded_total = 0

    def _stream() -> Generator[PointStruct, None, None]:
        nonlocal yielded_total
        limit = args.limit if args.limit and args.limit > 0 else None
        for pt in iter_points_from_sqlite(db_path=args.db, fetch_size=max(args.batch * 5, 1000), max_rows=limit):
            yielded_total += 1
            yield pt

    stream = _stream()

    # Prime dimension (peek 1 point)
    try:
        first = next(stream)
    except StopIteration:
        _eprint("‚ùå No points found in SQLite")
        sys.exit(2)

    buffer.append(first)
    if dim <= 0:
        dim = len(first.vector) if isinstance(first.vector, list) else 384
    print(f"üìê Using vector dimension: {dim}")

    # Ensure collection exists (and matches dim)
    _ensure_collection(cli, args.collection, dim, replace_existing=args.replace)

    # Upload: include primed buffer, then continue streaming in batches
    uploaded = 0
    batch_size = max(50, int(args.batch))

    # Fill buffer to a full batch first
    try:
        while len(buffer) < batch_size:
            buffer.append(next(stream))
    except StopIteration:
        pass

    if buffer:
        _retry_upsert(cli, args.collection, buffer)
        uploaded += len(buffer)
        print(f"   ‚Ä¢ Uploaded {uploaded} points")

    # Continue with the rest
    for batch in _batched(stream, batch_size):
        _retry_upsert(cli, args.collection, batch)
        uploaded += len(batch)
        if uploaded % (batch_size * 5) == 0:
            print(f"   ‚Ä¢ Uploaded {uploaded} points‚Ä¶")

    print(f"‚úÖ Upload complete: {uploaded} points total")

    if args.skip_verify:
        return

    # Lightweight verification (no embedding download required)
    try:
        info = cli.get_collection(args.collection)
        total_points = int(info.points_count or 0)
        print(f"üîé Cloud collection count: {total_points}")

        # Try a simple search with a zero vector of matching dimension
        zero_vec = [0.0] * dim
        res = cli.search(collection_name=args.collection, query_vector=zero_vec, limit=3)
        print(f"üîé Search sanity check returned: {len(res)} hits")
        if res:
            sample = res[0]
            print(f"   ‚Ä¢ sample score={getattr(sample, 'score', None)} payload_keys={list((sample.payload or {}).keys())}")
        print("üéâ Verification succeeded.")
    except Exception as e:
        _eprint(f"‚ö†Ô∏è Verification encountered an issue: {e}")
        _eprint("   (Upload likely succeeded; you can re-run without --skip-verify to try again.)")


if __name__ == "__main__":
    main()
